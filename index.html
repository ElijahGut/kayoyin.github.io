<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Kayo Yin</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />

        <script language="javascript">
            function show( elem ) {
                var x = document.getElementById(elem);
                if (x.style.display === "block") {x.style.display = "none";}
                else {x.style.display = "block";}
            }
        </script>
    </head>
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="#page-top">Kayo Yin</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#research">Research</a></li>
                        <li class="nav-item"><a class="nav-link" href="#pubs">Publications</a></li>
                        <li class="nav-item"><a class="nav-link" href="#talks">Talks</a></li>
                        <li class="nav-item"><a class="nav-link" href="media.html">Personal</a></li>
                        <li class="nav-item"><a class="nav-link" href="https://medium.com/@kayo.yin">Blog</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
            <section class="about-section text-center" id="about">
            <div class="container d-flex align-items-center justify-content-center">
                <div class="d-flex justify-content-center">
                    
                    <div class="text-center">
                        <div class="row">
                            <div class="smallcol"><script type="text/javascript">
                                ImageArray = new Array();
                                ImageArray[0] = 'assets/img/flute.png';
                                ImageArray[1] = 'assets/img/erhu.jpg';
                                ImageArray[2] = 'assets/img/piano.jpeg';
                                ImageArray[3] = 'assets/img/saxophone.png';
                            
                            function getRandomImage() {
                                var num = Math.floor( Math.random() * 4);
                                var img = ImageArray[num];
                                return('<img src="' + img + '" width="250px">')
                            
                            }
                            document.write(getRandomImage());
                            </script>
                            <br>
                            <h3 class="text-body mx-auto mt-2 mb-5">Hi, I'm Kayo <br>
                                Bonjour, je suis Kayo<br>
                                ‰ªäÊó•„ÅØ„ÄÅ„Åã„Çà„Å®Áî≥„Åó„Åæ„Åô„ÄÄ<br>
                                „ÄÄ‰Ω†Â•ΩÔºåÊàëÊòØÊÆ∑Á∂∫Â¶§</h3>
                                <tt>[kajo i…¥] </tt><br>
                                <tt>she/her </tt><br>
                                
                                <tt>kayoyü•∏cs.cmu.edu</tt>
                                                 <br>
                                     
                                                 <a href="assets/cv.pdf"><tt>Curriculum Vitae</tt></a> <br>
                                       <a href="https://scholar.google.com/citations?user=Wc8oLVwAAAAJ&hl=en"><img src="assets/img/scholar.png" width="30" height="30"></a>
                                       <a href="https://twitter.com/kayo_yin" title="Twitter"><img src="assets/img/twitter.png" width="30" height="30"></a>
                                       <a href="https://www.linkedin.com/in/kayoyin/" title="Linkedin"><img src="assets/img/linkedin.png" width="30" height="30"></a>
                                       <a href="https://github.com/kayoyin" title="Github"><img src="assets/img/github.png" width="30" height="30"></a> <br> <br>

                                              
                                     </ul>
                            
                            </div>
                           
                                <div class="bigcol">
                                    <div class="text-start">

                                    I am a Master's student in Language Technologies at <a href="https://www.lti.cs.cmu.edu"> Carnegie Mellon University</a> fortunately advised by <a href="http://www.phontron.com/">Prof. Graham Neubig</a> and researching multilingual NLP in <a href="http://www.cs.cmu.edu/~neulab/"> NeuLab</a>. I received my Bachelor's degree in Mathematics & Computer Science with highest distinction from <a href="https://www.polytechnique.edu/en">√âcole Polytechnique</a> where I defended my thesis on <a href="https://github.com/kayoyin/transformer-slt">Sign Language Translation</a>.
                                <br><br>
                                 I come from <a href="https://youtu.be/azpuIIz3mCU">Akashi, Japan</a> and grew up speaking 4 languages in <a href="https://youtu.be/_r4sBv4m0aI">Paris, France</a>. For as long as I can remember, I've had a passion for <a href="assets/img/animath.png">mathematics</a>. I actually learned to do multiplications and divisions before I learned to read!
                                 <a href="media.html">Music</a> is also a life-long passion of mine, and also what introduced me to AI: my worlds collided when I used LSTMs to generate music for my second-year programming <a href="http://github.com/kayoyin/prodigy">project</a>. When I discovered natural language processing, everything - my love of languages, numbers and codes - seemed to have fallen into place :)
                                 <br><br>
                                 I will be applying to PhD programs in Fall 2021!
                                 <br><br><br>
                                 <li>
                                  <b>2021-07-25</b> 1 paper accepted to the <a href="https://sites.google.com/tilburguniversity.edu/at4svl2021/home?authuser=0">AT4SSL workshop</a> at MT Summit 2021!
                                 </li>
                                 <li>
                                  <b>2021-07-05</b> Extremely thrilled to receive the <b>Best Theme Paper</b> award at <a href="https://2021.aclweb.org/">ACL 2021</a>!
                                 </li>
                                    <li>
                                     <b>2021-05-06</b> Super excited to have 3 papers accepted to <a href="https://2021.aclweb.org/">ACL 2021</a>!
                                 </li>
                                 <li>
                                  <b>2021-03-01</b> Gave an invited talk at Unbabel on "Do Context-Aware Translation Models Pay the Right Attention?"
                              </li>
                                 <li>
                                  <b>2020-10-18</b> Gave an invited talk at Computer Vision Talks on <a href="https://youtu.be/E5nKeEvoAK0">Sign Language Translation with Transformers</a>
                               </li>
                                 <li>
                                    <b>2020-09-30</b> My first conference submission was accepted to <a href="https://coling2020.org/"> COLING'2020</a>!
                                 </li>
                                 <li>
                                    <b>2020-09-21</b> Extremely honored to be awarded <b>Global Winner in Computer Science</b> at <a href="https://undergraduateawards.com/winners/global-winners-2020">The Global Undergraduate Awards 2020</a>!
                                 </li>
                                 <li>
                                  <b>2020-08-31</b>  Started my Master's degree at CMU LTI!
                               </li>
                                 <li>
                                    <b>2020-07-25</b>  My undergraduate paper has been accepted to the <a href="https://slrtp.com/">SLRTP workshop</a> at ECCV'20!
                                 </li>
                                 </div>
                                </div>
                          </div>
                        
             
                    </div>
                </div>
            </div></section>

            <section class="about-section" id="research">
                <div class="container ">
                    

                <h6 class="display-6">Research</h6>
                <hr>
                My long-term research goal is to break down communication barriers between people, and between computers and people using Natural Language Processing. I believe that by improving how computers understand and respond to different natural languages, more people can benefit from technology using their preferred language.
                <br> <br>
                Currently, I am working on challenges in multilingual NLP such as: <br> <br>
                 <li><b>Context-aware Machine Translation</b>: I am interested in when context, either on an intra-sentential (within the current sentence), inter-sentential (across multiple sentences), or extra-linguistic (e.g. social, temporal, cultural) level, is required during translation, and how to model these features in machine translation. 
                  My works examine how much context-aware models are actually using context (<a href="https://aclanthology.org/2021.acl-long.505/">ACL'21</a>), whether models are using the type of context we expect them to (<a href="https://aclanthology.org/2021.acl-long.65/">ACL'21</a>), and how to encourage models to use more/better context. </li> <br>
                  
                 <li><b>Sign Language Processing</b>: I am interested in modeling signed languages from a linguistic perspective and extending existing language technologies to signed languages.
                  I have argued the importance for the NLP community to include signed languages, both socially and scientifically, and how to get involved (<a href="https://aclanthology.org/2021.acl-long.570/">ACL'21</a>), I researched how to translate a signed language into a spoken language (<a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">ECCV'20</a>, <a href="https://www.aclweb.org/anthology/2020.coling-main.525/">COLING'20</a>),
                  and how to perform data augmentation for Sign Language Translation (<a href="https://arxiv.org/abs/2105.07476">MTSummit21</a>). </li>
                <br><br>

                Please do not hesitate to reach out if you'd like to chat or collaborate! I am generally responsive to emails and Twitter messages, I am not as responsive as I'd like to be on LinkedIn.
               </div></div>


            </section>

            <section class="about-section" id="pubs">
                <div class="container">

                <h3 class="display-6">Publications</h3>
                <hr>
                <i>* = equal contribution</i>
                <br><br>
                (You may have to click on "Abstract" and "BibTeX" twice for them to show. If any webdev-savvy people can look at the source code and see how I can fix this lmk!)
                <br><br>
                 <h4>2021</h4>     
                 <ul class="clist">

                  <li> 
                    <a href="https://arxiv.org/abs/2105.07476"> Data Augmentation for Sign Language Gloss Translation</a>
                    <br>
                    Amit Moryossef*, <u>Kayo Yin*</u>,  Graham Neubig and Yoav Goldberg.
                      <br>
                      <i>18th Biennial Machine Translation Summit (MTSummit) 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL). August 2021.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs4')">
                        Abstract
                      </button>
                      <a href="https://arxiv.org/pdf/2105.07476.pdf"><span class="badge badge-pdf"> PDF </span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib4')">
                        BibTeX
                      </button>
        
                      <div id="abs4"  class="popup-abs">
                          <br>
                        &nbsp;&nbsp; Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.
                        <br><br></div>
                      
                      <div id="bib4"  class="popup">
                         <br>
                         @article{moryossef2021data, <br>
                            &nbsp;&nbsp;&nbsp;&nbsp;title={{Data Augmentation for Sign Language Gloss Translation}},<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;author={Moryossef, Amit and Yin, Kayo and Neubig, Graham and Goldberg, Yoav},<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;journal={First International Workshop on Automatic Translation for Sign and Spoken Languages.},<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;year={2021},<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;url      = {https://arxiv.org/abs/2105.07476},<br>
                          }<br><br>
                      </div><br><br>



                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.65/"> Do Context-Aware Translation Models Pay the Right Attention? </a>
                <br>
                <u>Kayo Yin</u>, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, Andr√© F. T. Martins and Graham Neubig.
                 <br>
                  <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                  <br>
                  <button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs1')">
                Abstract
              </button>
              <a href="https://aclanthology.org/2021.acl-long.65.pdf"><span class="badge badge-pdf"> PDF</span></a>
                  <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">  Code/Data </span></a>
                  <a href="https://youtu.be/IRiy_xpWC-0"> <span class="badge badge-video">Video</span></a>
                <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib1')">
                BibTeX
              </button>

              <div id="abs1"  class="popup-abs">
                  <br>
                &nbsp;&nbsp; Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.
                <br><br></div>
              
              <div id="bib1"  class="popup">
                 <br>
                 @inproceedings{yin-etal-2021-context,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; title = "Do Context-Aware Translation Models Pay the Right Attention?",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; author = "Yin, Kayo  and
                    Fernandes, Patrick  and
                    Pruthi, Danish  and
                    Chaudhary, Aditi  and
                    Martins, Andr{\'e} F. T.  and
                    Neubig, Graham",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.65",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;pages = "788--801",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model{'}s attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.",<br>
              }<br><br>
              </div><br><br>

                 <li> 
                  <span class="badge badge-sp"> <b>Best Theme Paper</b> </span> <a href="https://aclanthology.org/2021.acl-long.570/">Including Signed Languages in Natural Language Processing</a> 
                <br>
                <u>Kayo Yin</u>, Amit Moryossef, Julie Hochgesang, Yoav Goldberg and Malihe Alikhani.
                  <br>
                  <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs2')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.acl-long.570.pdf"><span class="badge badge-pdf"> PDF </span></a>
                  <a href="https://youtu.be/AYEIcOsUyWs"> <span class="badge badge-video">Video</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib2')">
                    BibTeX
                  </button>
    
                  <div id="abs2"  class="popup-abs">
                      <br>
                    &nbsp;&nbsp; Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.
                    <br><br></div>
                  
                  <div id="bib2"  class="popup">
                     <br>
                     @inproceedings{yin-etal-2021-including,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;title = "Including Signed Languages in Natural Language Processing",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                        Moryossef, Amit  and
                        Hochgesang, Julie  and
                        Goldberg, Yoav  and
                        Alikhani, Malihe",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.570",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "7347--7360",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",<br>
                  }<br><br>
                  </div><br><br>



                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.505/"> Measuring and Increasing Context Usage in Context-Aware Machine Translation </a>
                <br>
                  Patrick Fernandes, <u>Kayo Yin</u>, Graham Neubig and Andr√© F. T. Martins.
                 <br>
                  <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs3')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.acl-long.505.pdf"><span class="badge badge-pdf"> PDF</span></a>
                      <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">Code/Data</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib3')">
                    BibTeX
                  </button>
    
                  <div id="abs3"  class="popup-abs">
                      <br>
                    &nbsp;&nbsp; Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.
                    <br><br></div>
                  
                  <div id="bib3"  class="popup">
                     <br>
                     @inproceedings{fernandes-etal-2021-measuring,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;title = "Measuring and Increasing Context Usage in Context-Aware Machine Translation",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Fernandes, Patrick  and
                        Yin, Kayo  and
                        Neubig, Graham  and
                        Martins, Andr{\'e} F. T.",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.505",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "6467--6478",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",<br>
                  }<br><br>
                  </div><br><br>
               </ul>


                  
      
                <h4>2020</h4>
                          <ul class="clist">
                 <li> 
                  <a href="https://www.aclweb.org/anthology/2020.coling-main.525/">Better Sign Language Translation with STMC-Transformer</a>
                <br>
                <u>Kayo Yin</u> and Jesse Read.
                  <br>
                  <i>Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020).</i> 

                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs5')">
                    Abstract
                  </button>
                  <a href="https://www.aclweb.org/anthology/2020.coling-main.525.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code/Data</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib5')">
                    BibTeX
                  </button>
    
                  <div id="abs5"  class="popup-abs">
                      <br>
                    &nbsp;&nbsp; Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU. We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.
                <br><br></div>
                  
                  <div id="bib5"  class="popup">
                     <br>
                     @inproceedings{yin-read-2020-better, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title = "Better Sign Language Translation with {STMC}-Transformer", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and Read, Jesse", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 28th International Conference on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = December, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2020", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Barcelona, Spain (Online)",
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "International Committee on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://www.aclweb.org/anthology/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;doi = "10.18653/v1/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "5975--5989", <br>
                    }<br><br>
                  </div> <br><br>

                 <li> <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">
                  Attention is All You Sign: Sign Language Translation with Transformers
                  </a>
                <br>
                <u>Kayo Yin</u> and Jesse Read.
                 <br>
                  <i>European Conference on Computer Vision (ECCV) Workshop on Sign Language Recognition, Translation and Production (SLRTP).</i> 
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs6')">
                    Abstract
                  </button>
                  <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code/Data</span></a>
                      <a href="https://youtu.be/c-87jFd2lQs"> <span class="badge badge-video">Video</span></a>

                   
                      <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib6')">
                    BibTeX
                  </button>
    
                  <div id="abs6"  class="popup-abs">
                      <br>
                    &nbsp;&nbsp; This paper improves the translation system in Sign Language
                    Translation (SLT) by using Transformers. We report a wide range of experimental results for various Transformer setups and introduce a novel
                    end-to-end SLT system combining Spatial-Temporal Multi-Cue (STMC)
                    and Transformer networks. Our methodology improves on the current
                    state-of-the-art by over 5 and 7 BLEU respectively on ground truth
                    (GT) glosses and predicted glosses of the PHOENIX-Weather 2014T
                    dataset. On the ASLG-PC12 corpus, we report an improvement of over
                    16 BLEU. Our findings also reveal that end-to-end translation with predicted glosses outperforms translation on GT glosses. This shows the
                    potential for further improvement in SLT by either jointly training the
                    SLR and translation systems or by revising the gloss annotation scheme. <br><br></div>
                  
                  <div id="bib6"  class="popup">
                     <br>
                     @inproceedings{yin2020attention,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title={{Attention is All You Sign: Sign Language Translation with Transformers}},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author={Yin, Kayo and Read, Jesse},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Sign Language Recognition, Translation and Production (SLRTP) Workshop-Extended Abstracts},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;volume={4},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year={2020}<br>
                      }<br><br>
                  </div> 
            
                </ul>

              <section id="talks" class="mt-5">
                <h3 class="display-6">Talks</h3>
                <hr>

                           
          <ul class="clist">
            <li> <span class="badge badge-abs">2020</span> <span class="badge badge-video">Talk</span> 
                Sign Language Translation with Transformers @UA Global Summit 
 
            <br><br>
 <iframe width="370" height="210" src="https://www.youtube.com/embed/MPit0Oh4reM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>           </ul>
</li> 
</ul>
<ul class="clist">

    <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
        <a href="https://programmes.polytechnique.edu/en/post/bachelor-of-science-alumni-kayo-at-carnegie-mellon-university"> Bachelor of Science Alumni - Kayo at Carnegie Mellon University</a>  
         </li> 
         I reminiscence my time at the Bachelor of l'X and talk about what I've been up to afterwards.
        </ul>
        <ul class="clist">
            <li> <span class="badge badge-abs">2020</span> <span class="badge badge-video">Talk</span> 
                 Sign Language Translation with Transformers @Computer Vision Talks 
 
            <br><br>
 <iframe width="370" height="210" src="https://www.youtube.com/embed/E5nKeEvoAK0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>           </ul>
</li> 
        <ul class="clist">
            <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://www.polytechnique.edu/en/content/graduate-lx-bachelor-science-kayo-yin-wins-2020-global-undergraduate-awards"> Graduate of l‚ÄôX Bachelor of Science, Kayo Yin wins the 2020 Global Undergraduate Awards</a>  
         </li> 
        I share what winning the Global Undergraduate Awards means to me, what my winning entry "Sign Language Translation with Transformers" is about and my ambitions for my current and next research. 
               </ul>
        <ul class="clist">
            <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/testimony-yin-kayo-bx2020-mathscomputer-science-student"> Testimony: Kayo Yin, BX2020 Maths/Computer Science Student</a>  
         </li> 
         I share my experience as part of the first cohort of Polytechnique's Bachelor program, which I highly recommend to any student with a strong background in math and passion for science.
        </ul>
        <ul class="clist">
            <li>  <span class="badge badge-abs">2019</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/women-in-science-meet-kayo-yin-bachelor-student">Women in Science - Meet a student from the Bachelor Program </a>  
         </li> 
         I briefly talk about my exchange in Taiwan, my part-time NLP research job. Most importantly, I give my views on the position of women in the scientific field today.
        </ul>
         
         <ul class="clist">
            <li>  <span class="badge badge-abs">2018</span> <span class="badge badge-secondary">Interview</span> 
                Meet our students - Kayo Yin </li> 
           I talk about how I am able to continue practicing music while pursuing my double-major in mathematics and computer science at l'X.
           <br><br>
           
         <iframe width="370" height="210" src="https://www.youtube.com/embed/RoZAu4pHCxg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </ul>
          
          <ul class="clist">
          <li>  <span class="badge badge-abs">2017</span> <span class="badge badge-secondary">Interview</span> 
         <a href="https://gargantua.polytechnique.fr/siatel-web/linkto/mICYYYTGHYK#page=20t">A Lifetime of Science - Kayo Yin, Femme-Orchestre </a>  
         </li>
           "A Lifetime of Science" is the 2017 annual report of √âcole Polytechnique featuring 20 selected student profiles. I was noticed during my first year of undergraduate for my investment in various extracurricular activities. 
         </ul>

        </div>
            </section>
            <br><br><br>
            <i> <p style="color:#808080">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last updated July 26 2021</p> </i>

        
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
